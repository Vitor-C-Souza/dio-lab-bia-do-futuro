import json
import pandas as pd
from dotenv import load_dotenv
import os
from google import genai
import streamlit as st

# Carregar chaves e configurar Gemini
load_dotenv()
api_key = os.getenv("GEMINI_API_KEY")
client = genai.Client(api_key=api_key)

# --- CARREGAMENTO DA BASE DE CONHECIMENTO ---
# Usando os arquivos que voc√™ forneceu para garantir que o Markin saiba de quem fala
perfil = json.loads(open('./data/perfil_investidor.json', encoding='utf-8').read())
transacoes = pd.read_csv('./data/transacoes.csv')
historico = pd.read_csv('./data/historico_atendimento.csv')
produtos = json.loads(open('./data/produtos_financeiros.json', encoding='utf-8').read())

# --- CONTEXTO E ANTI-ALUCINA√á√ÉO (SYSTEM PROMPT) ---
# Adicionamos regras r√≠gidas para o Markin n√£o inventar dados ou recomendar a√ß√µes espec√≠ficas.
SYSTEM_PROMPT = f"""
Voc√™ √© o "Financeiro Amigo Markin". Sua personalidade √© amig√°vel, consultiva e educativa.
Sua base de conhecimento √© restrita aos dados fornecidos abaixo.

REGRAS DE SEGURAN√áA E ANTI-ALUCINA√á√ÉO:
1. Responda APENAS com base nos dados do CLIENTE e nos PRODUTOS DISPON√çVEIS fornecidos.
2. Se o usu√°rio perguntar algo fora do contexto financeiro ou sobre dados que voc√™ n√£o possui, responda: "Desculpe, mas n√£o tenho informa√ß√µes suficientes para responder a isso. Posso ajudar com outra coisa?"
3. NUNCA invente transa√ß√µes, valores ou produtos que n√£o estejam na lista.
4. NUNCA recomende a√ß√µes espec√≠ficas (ex: PETR4, VALE3). Se perguntado, explique que voc√™ sugere categorias de investimento baseadas no perfil.
5. Se o cliente gastar mais do que ganha nas transa√ß√µes, seja proativo e sugira cautela de forma amig√°vel.

CONTEXTO ATUAL:
- CLIENTE: {perfil['nome']}, {perfil['idade']} anos, Perfil: {perfil['perfil_investidor']}.
- PATRIM√îNIO: R$ {perfil['patrimonio_total']} | RESERVA: R$ {perfil['reserva_emergencia_atual']}.
- TRANSA√á√ïES RECENTES: {transacoes.tail(5).to_dict(orient='records')}
- PRODUTOS PARA ESTE PERFIL: {json.dumps(produtos, ensure_ascii=False)}
"""

def perguntar(msg):
    prompt_completo = f"{SYSTEM_PROMPT}\n\nPergunta do Usu√°rio: {msg}"

    # Usando o modelo 2.0 Flash para maior velocidade e precis√£o
    response = client.models.generate_content(
        model="gemini-3-flash-preview",
        contents=prompt_completo
    )
    return response.text

# --- INTERFACE STREAMLIT ---
st.set_page_config(page_title="Amigo Markin", page_icon="üí∞")

# Sidebar com Dashboard em tempo real baseado nos seus CSVs
with st.sidebar:
    st.header("üìä Resumo de {perfil['nome']}")
    st.metric("Patrim√¥nio", f"R$ {perfil['patrimonio_total']}")
    
    # Gr√°fico r√°pido de Gastos por Categoria (Anti-Alucina√ß√£o Visual)
    st.subheader("Gastos por Categoria")
    gastos = transacoes[transacoes['valor'] < 0].groupby('categoria')['valor'].sum().abs()
    st.bar_chart(gastos)

st.title("üí∞ O Financeiro Amigo Markin")

if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": f"Ol√°! Sou o Markin. Vi aqui que seu objetivo √© {perfil['objetivo_principal']}. Como posso te ajudar hoje?"}]

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if pergunta := st.chat_input("Ex: 'Vale a pena investir em Renda Fixa?'"):
    st.session_state.messages.append({"role": "user", "content": pergunta})
    with st.chat_message("user"):
        st.markdown(pergunta)

    with st.chat_message("assistant"):
        with st.spinner("Consultando sua carteira..."):
            resposta = perguntar(pergunta)
            st.markdown(resposta)
            st.session_state.messages.append({"role": "assistant", "content": resposta})